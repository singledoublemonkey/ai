
1. 变量
    
    tf.Valiable

    变量初始化相关all关键字，全部改成global
    
    早期版本 VS 新版本(1.0.1)
    ```
    tf.initialize_all_variables() --> tf.global_variables_initializer()
    tf.all_variables()  --> tf.global_variables()
    ```

2. 常量
    
    tf.constant

3. 占位符
    tf.placeholder

4. 矩阵相乘
    tf.matmul

5. 概率分布化 —— softmax
    
    softmax回归本身可作为一个学习算法优化分类结果。在tf中，回归参数被去掉，仅是一个额外处理层，将神经网络的输出变为概率分布。
    
6. 交叉熵
    
    通过q来表示p的交叉熵    
    公式：
    
    ![image](http://chart.googleapis.com/chart?cht=tx&chl=\Large%20H(p,q)=-%20\sum_%20x%20p(x)log%20q(x))

    交叉熵用来刻画两个概率分布之间的距离。交叉熵越小，两个概率分布越接近。
    
    交叉熵的函数不是对称的![image](http://chart.googleapis.com/chart?cht=tx&chl=\Large%20H(p,q)%20\neq%20H(q,p))，在神经网络中，p代表正确结果，而q表示预测结果。
    
7. 过滤限定计算结果函数

    tf.clip_by_value(x, min, max)
    
    此函数会将一个Tensor中的数值限定在一个范围内，可以避免运算错误，如 `$log0$` 是无效的。x表示一个tensor，min即表示tensor中小于此值的数将采用min运算，max表示tensor中大于此值的数将采用max运算。

8. 计算对数

    tf.log：此函数完成了依次对tensor中所有元素求对数的功能。
    
9. 比较函数
    
    tf.greater(x, y)，可以比较矩阵中对应元素的大小。
    
10. 选择函数
    
    在之前的tensorflow版本中选择函数是 tf.select ，但是在新版本中（比如：1.0.1）select函数已经废弃了，采用where函数来替代select函数。tf.where(condation, x, y),
表示当条件为true时，执行x，为false时，执行y。

    **code：**
    
    ```
    import tensorflow as tf
    
    v1 = tf.constant([1, 2, 3, 4], tf.float32)
    v2 = tf.constant([4, 3, 2, 1], tf.float32)
    
    with tf.Session() as sess:
        # [False False  True  True]
        print(tf.greater(v1, v2).eval())
        # [ 4.  3.  3.  4.]
        print(tf.where(tf.greater(v1, v2), v1, v2).eval())
    ```

11. 队列

    tensorflow提供了2种类型的队列，FIFOQueue和RandomShuffleQueue。tensorflow中，队列不仅仅是数据结构，还是异步计算张量取值的一个重要机制。

12. 多线程协同

    tf.Coordinator和tf.QueueRunner完成多线程协同功能。
    
    tf.Coordinator主要协同多个线程一起停止。提供了should_stop、request_stop和join三个函数。
    
    tf.QueueRunner主要用于启动多个线程操作同一个队列，启动的线程可以通过coordinator来管理。

13. tensor和numpy转换

    Tensorflow中提供了Tensor和SparseTensor，SparseTensor指的是稀疏张量，即元素大多数值为0，只有少数是有意义的值。
    
    **tensor转换为numpy：**

    ```
    # 定义一个sparse tensor， 
    sparse_tensor = tf.SparseTensor(
        dense_shape=[4, 5],         # 二维的4x5
        indices=[[2, 1], [3, 4]],   # 不为空值的下标
        values=[3.0, 8.0]           # 不为空的值
    )
    
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        # 转换sparse tensor为tensor
        dense_tensor = tf.sparse_tensor_to_dense(sp_input=sparse_tensor, validate_indices=True)
        # 获取sparse tensor输出
        print(sparse_tensor.eval())
        # 打印转换后的numpy
        print(dense_tensor.eval())
    ```
    输出：

    ```
    SparseTensorValue(indices=array([[2, 1],
           [3, 4]], dtype=int64), values=array([ 3.,  8.], dtype=float32), dense_shape=array([4, 5], dtype=int64))
    [[ 0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.]
     [ 0.  3.  0.  0.  0.]
     [ 0.  0.  0.  0.  8.]]
    ```
    **numpy转换为tensor：**
    
    ```
    #定义一个numpy的矩阵，然后将其转换为tensor
    mat = np.mat(data=[[3.0, 1.8, 4.5], [5.0, 2.6, 1.9]], dtype=np.float32)
    print(tf.constant(mat))
    ```
    输出：
    
    ```
    Tensor("Const:0", shape=(2, 3), dtype=float32)
    ```

