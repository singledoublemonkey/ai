
### 优化方式

- 神经网络结构设计：激活函数、多隐层
- 神经网络训练设计：指数衰减学习率、正则化损失函数、滑动平均模型
- 图像神经网络：卷积神经网络

### 激活函数

激活函数主要为了对神经网络进行去线性化。常用的激活函数主要有：
- ReLU：即大于0的数，位于第一象限内的射线
- Sigmod：sigmod函数是值位于(0,1)之间的函数，公式：![image](https://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B1&plus;e%5E%7B-x%7D%7D)
- tanh：双曲函数，公式：![image](https://latex.codecogs.com/gif.latex?%5Cfrac%7B1-e%5E%7B-2x%7D%7D%7B1&plus;e%5E%7B-2x%7D%7D)

### 优化函数：

梯度下降、随机梯度下降

梯度下降法：可能收敛于局部最优解，且需要再全量数据上进行计算导致计算时间较长。

随机梯度下降法：在每一轮迭代中，随机优化某一条数据上的损失函数，参数更新速度变快，训练速度变快。但是某一条数据的损失并不代表整体的损失度，所以可能连局部最优都无法收敛。

### 损失函数

交叉熵损失。

一般采用梯度下降法来优化最小交叉熵损失、或者优化交叉熵和正则化损失的和

### 数据层：

- 小批量随机训练数据 SGD+batch
- shuffle训练样本
- 标准化数据：均值为0，方差为1进行数据标准化

### 学习率优化

学习率用来控制参数的更新速度。

学习率采用非固定方式，而是采用指数衰减的方式来设定。指数衰减可以让模型在前期以更快的方式接近较优解，后期不会有太大的波动，从而更加接近局部最优解。

指数衰减，在tensorflow中，是通过tf.train.exponential_decay函数来使用。初始学习率、衰减系数、衰减速度都是根据经验值进行设置。

### 过拟合
对于复杂模型而言训练会出现过拟合问题。

过拟合会导致泛化能力变差。**主要原因：** 过拟合过度关注了训练数据集中的噪声分布，而忽视了问题的整体规律。导致对训练数据的刻画过于完美，而对未知的数据无法更好的做判断。

避免过拟合，常用的方法是正则化（regularization）、dropout（训练过程中通过随机下线一定比率数据来防止过拟合）

### 正则化

正则化思想：在损失函数中加入刻画模型复杂度的指标。

比如：模型损失函数为![image](https://latex.codecogs.com/gif.latex?J%28%5Ctheta%29)，那么采用正则化的方式，在优化时，就不是直接优化![image](https://latex.codecogs.com/gif.latex?J%28%5Ctheta%29)，而是优化正则化后的函数。

正则化后的函数：
![image](https://latex.codecogs.com/gif.latex?J%28%5Ctheta%29&plus;%5Clambda%20R%28w%29)
>其中，![image](https://latex.codecogs.com/gif.latex?R%28w%29)刻画的是模型的复杂度，而![image](https://latex.codecogs.com/gif.latex?%5Clambda)表示模型复杂损失在总体损失中的比例。![image](https://latex.codecogs.com/gif.latex?J%28%5Ctheta%29)是神经网络中所有参数，包括权重![image](https://latex.codecogs.com/gif.latex?w)和偏置![image](https://latex.codecogs.com/gif.latex?b)。但是一般模型复杂度由权重决定。

正则化函数：

L1正则化：
![image](https://latex.codecogs.com/gif.latex?R%28w%29%3D%5Cparallel%20w%5Cparallel_1%20%3D%20%5Csum_%7Bi%7D%5Cmid%20w_i%5Cmid)

即将权重按绝对值后求和。
>L1正则会让参数变得更稀疏，稀疏是指会有更多的参数变为0，可以达到类似特征提取的功能。L1正则是不可导的。

L2正则化：
![image](https://latex.codecogs.com/gif.latex?R%28w%29%3D%5Cparallel%20w%5Cparallel_2%5E2%20%3D%20%5Csum_%7Bi%7D%5Cmid%20w_i%5E2%5Cmid)

即将权重平方后求和。
> L2不会出现L1的情况，主要是当参数很小时，比如0.001，平方后基本可以忽略，模型不会进一步将此参数调整为0。L2正则是可导函数。

实际中，可以将L1和L2正则同时使用：

![image](https://latex.codecogs.com/gif.latex?R%28w%29%3D%5Csum_i%20%5Calpha%5Cmid%20w_i%5Cmid%20&plus;%20%281-%5Calpha%29%20w_i%5E2%5Cmid)

tensorflow中提供了对应的函数来表示L1和L2。

```
L1 -> tf.contrib.layers.l1_regularizer(lambda)(w)
L2 -> tf.contrib.layers.l2_regularizer(lambda)(w)
```

### 滑动平均模型
    
此模型可以是模型在测试数据集上更加健壮。即在一定程度上可以提升模型在测试数据集上的表现能力。

tf.train.ExponentialMovingAverage，需要提供一个衰减率，来控制模型的更新速度。实际中，衰减率一般会设置成为接近1的数（比如0.99或0.9999）

变量的滑动平均值是通过影子变量维护。


### 优化比对

滑动模型和指数衰减都是在一定程度上限制神经网络参数的更新速度。在复杂问题中，迭代收敛的速度不会很快，这时滑动模型和指数衰减优化会发挥更大的作用。正则化损失函数优化的效果较为显著，可以降低一定比率的错误率（如6%）。

对于损失函数优化，仅采用最小交叉熵、和采用最小交叉熵+正则化来说，后者的优化程度会表现的更好，会较好的避免过拟合问题。

主要表现在：仅采用最小交叉熵在训练集中，也许会有更小的交叉熵值，表现更好，但是在测试数据集上会稍差于最小交叉熵+正则化的预测准确率。也就是仅采用最小交叉熵的方法会存在过于完美和刻画训练数据，而忽视了数据的潜在分布规律。
